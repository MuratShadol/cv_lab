# -*- coding: utf-8 -*-
"""image_classification_transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wy3Bq5fdN0o5P0KsGUvbZZlGJvWlp6Oq
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from keras.models import Sequential
from keras import layers

TRAIN_PATH = '/content/drive/MyDrive/DataSets/Classification_data/train'
TEST_PATH = '/content/drive/MyDrive/DataSets/Classification_data/test'

batch_size = 32
img_height = 150
img_width = 150

def main():
  train_ds = create_train_set(TRAIN_PATH)
  val_ds = create_val_set(TEST_PATH)
  class_names = train_ds.class_names
  num_classes = len(class_names)
  train_ds, val_ds = preprocess_data(train_ds, val_ds)
  data_augmentation = Sequential(
    [layers.RandomFlip("horizontal"), layers.RandomRotation(0.1),]
  )
  base_model = get_base_model()
  callback = get_callback()
  model = train_top_layer(base_model, train_ds, val_ds, callback, data_augmentation)
  train_entire_model(base_model, model, train_ds, val_ds, 1000, callback)


def create_train_set(TRAIN_PATH):
  train_ds = tf.keras.utils.image_dataset_from_directory(
  TRAIN_PATH,
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
  return train_ds

def create_val_set(TEST_PATH):
  val_ds = tf.keras.utils.image_dataset_from_directory(
  TEST_PATH,
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
  return val_ds


def preprocess_data(train_ds, val_ds):
  # Use buffered prefetching, so you can yield data from disk without having I/O become blocking.
  AUTOTUNE = tf.data.AUTOTUNE
  train_ds = train_ds.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE)
  val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
  return train_ds, val_ds



def get_base_model():
  base_model = keras.applications.Xception(
      weights='imagenet',  # Load weights pre-trained on ImageNet
      input_shape=(150, 150, 3),
      include_top=False,
      classes=6
  )
  return base_model

# Freeze the base_model
def train_top_layer(base_model, train_ds, val_ds, callback, data_augmentation):
  base_model.trainable = False

  # Create new model on top
  inputs = keras.Input(shape=(150, 150, 3))
  x = data_augmentation(inputs) # apply random data augmentation

  # Rescale the data to [-1; 1] for Xception
  scale_layer = layers.Rescaling(scale=1 / 127.5, offset=-1)
  x = scale_layer(x)

  # The base model contains batchnorm layers. We want to keep them in inference mode
  # when we unfreeze the base model for fine-tuning, so we make sure that the
  # base_model is running in inference mode here
  x = base_model(x, training=False)
  x = keras.layers.GlobalAveragePooling2D()(x)
  x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
  outputs = keras.layers.Dense(6, activation='softmax')(x)
  model = keras.Model(inputs, outputs)
  model.summary()
  model.compile(optimizer="adam",
              loss=keras.losses.SparseCategoricalCrossentropy(),
              metrics=["accuracy"])

  # Train the top layer
  EPOCHS = 100
  model.fit(train_ds,
            epochs=EPOCHS,
            validation_data=val_ds,
            callbacks=callback)
  return model

def get_callback():
  # Initialize early stopping to prevent overfitting
  callback = keras.callbacks.EarlyStopping(
      monitor='val_loss',
      min_delta=0,
      patience=10,
      verbose=0,
      mode='auto',
      baseline=None,
      restore_best_weights=True,
      start_from_epoch=0
  )
  return callback

def train_entire_model(base_model, model, train_ds, val_ds, EPOCHS, callback):
# Unfreeze the base model and train the entire model end-to-end with a low learning rate
  base_model.trainable = True
  model.summary()

  model.compile(
      optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate
      loss=keras.losses.SparseCategoricalCrossentropy(),
      metrics=["accuracy"]
  )

  model.fit(train_ds,
            epochs=EPOCHS,
            validation_data=val_ds,
            callbacks=callback)

main()

val_loss_top = np.array([0.3177, 0.2946, 0.283 , 0.2899, 0.2787, 0.2753, 0.3028, 0.2847,
       0.2751, 0.2806, 0.2732, 0.2892, 0.2766, 0.2774, 0.2822, 0.2735,
       0.2838, 0.2664, 0.2765, 0.2778, 0.2889, 0.3056, 0.2816, 0.2675,
       0.2963, 0.2699, 0.2783, 0.2768])

val_acc_top = np.array([0.8827, 0.8971, 0.8927, 0.8921, 0.8991, 0.8987, 0.8884, 0.8994,
       0.8977, 0.8981, 0.9021, 0.8961, 0.9027, 0.8971, 0.8987, 0.9034,
       0.9004, 0.8991, 0.9031, 0.9011, 0.8877, 0.8901, 0.8981, 0.9031,
       0.8957, 0.9057, 0.8984, 0.9041])

val_loss_all = np.array([0.2361, 0.2101, 0.2013, 0.1969, 0.2058, 0.1974, 0.204 , 0.2182,
       0.2205, 0.2148, 0.2378, 0.2337, 0.2425, 0.2453])

val_acc_all = np.array([0.9154, 0.9177, 0.9277, 0.9277, 0.9284, 0.929 , 0.928 , 0.9314,
       0.9287, 0.931 , 0.9317, 0.9257, 0.9247, 0.9317])
epochs_range_top = range(len(val_loss_top))
epochs_range_all = range(len(val_acc_all))

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range_top, val_acc_top, label='Accuracy of top layer')
plt.plot(epochs_range_top, val_loss_top, label='Loss of top layer')
plt.legend(loc='center right')
plt.xlabel("Epochs")
plt.title('Accuracy and loss of top layer')

plt.subplot(1, 2, 2)
plt.plot(epochs_range_all, val_acc_all, label='Accuracy of entire model')
plt.plot(epochs_range_all, val_loss_all, label='Loss of entire model')
plt.legend(loc='center right')
plt.xlabel("Epochs")
plt.title('Accuracy and loss of the entire model')
plt.show()

import matplotlib.pyplot as plt

val_loss

